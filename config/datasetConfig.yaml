unet_target_modules:
  - "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q"
  - "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v"
  - "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q"
  - "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v"
  - "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q"
  - "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v"
  - "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q"
  - "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v"
  - "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q"
  - "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v"
  - "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q"
  - "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v"
  - "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q"
  - "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v"
  - "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q"
  - "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v"
  - "down_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q"
  - "down_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v"
  - "down_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q"
  - "down_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v"
  - "down_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q"
  - "down_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v"
  - "down_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q"
  - "down_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v"
  - "up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q"
  - "up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v"
  - "up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q"
  - "up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v"
  - "up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q"
  - "up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v"
  - "up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q"
  - "up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v"
  - "up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q"
  - "up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v"
  - "up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q"
  - "up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v"
  - "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q"
  - "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v"
  - "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q"
  - "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v"
  - "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q"
  - "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v"
  - "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q"
  - "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v"
  - "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q"
  - "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v"
  - "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q"
  - "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v"
  - "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q"
  - "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v"
  - "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q"
  - "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v"
  - "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q"
  - "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v"
  - "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q"
  - "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v"
  - "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q"
  - "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v"
  - "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q"
  - "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v"
  - "mid_block.attentions.0.transformer_blocks.0.attn1.to_q"
  - "mid_block.attentions.0.transformer_blocks.0.attn1.to_v"
  - "mid_block.attentions.0.transformer_blocks.0.attn2.to_q"
  - "mid_block.attentions.0.transformer_blocks.0.attn2.to_v"

vae_target_modules:
  - "encoder.mid_block.attentions.0.to_q"
  - "encoder.mid_block.attentions.0.to_v"
  - "decoder.mid_block.attentions.0.to_q"
  - "decoder.mid_block.attentions.0.to_v"

rank: 4
alpha: 8
learning_rate: 0.0001
max_train_steps: 20
