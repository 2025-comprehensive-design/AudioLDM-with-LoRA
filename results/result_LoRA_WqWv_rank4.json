{
    "experiment": "LoRA_WqWv_rank4",
    "targets": [
        "text_model.encoder.layer.0.attention.self.query",
        "text_model.encoder.layer.1.attention.self.query",
        "text_model.encoder.layer.2.attention.self.query",
        "text_model.encoder.layer.3.attention.self.query",
        "text_model.encoder.layer.4.attention.self.query",
        "text_model.encoder.layer.5.attention.self.query",
        "text_model.encoder.layer.6.attention.self.query",
        "text_model.encoder.layer.7.attention.self.query",
        "text_model.encoder.layer.8.attention.self.query",
        "text_model.encoder.layer.9.attention.self.query",
        "text_model.encoder.layer.10.attention.self.query",
        "text_model.encoder.layer.11.attention.self.query",
        "text_model.encoder.layer.0.attention.self.value",
        "text_model.encoder.layer.1.attention.self.value",
        "text_model.encoder.layer.2.attention.self.value",
        "text_model.encoder.layer.3.attention.self.value",
        "text_model.encoder.layer.4.attention.self.value",
        "text_model.encoder.layer.5.attention.self.value",
        "text_model.encoder.layer.6.attention.self.value",
        "text_model.encoder.layer.7.attention.self.value",
        "text_model.encoder.layer.8.attention.self.value",
        "text_model.encoder.layer.9.attention.self.value",
        "text_model.encoder.layer.10.attention.self.value",
        "text_model.encoder.layer.11.attention.self.value"
    ],
    "rank": 4,
    "alpha": 8,
    "clap_score": 0.595866222307086,
    "loss": 0.04268662768881768
}